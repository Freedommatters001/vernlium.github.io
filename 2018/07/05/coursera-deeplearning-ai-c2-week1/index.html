<!DOCTYPE html>




<html class="theme-next pisces" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Consolas:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="本周课程主要讲解了神经网络中的优化的一些方法，要点：  Train/Dev/Test（训练/开发/测试数据集） 偏差和方差 欠拟合和过拟合 Regularization正则化 Dropout随机失活 Normalizing归一化 梯度消失和梯度爆炸 梯度检查  本周课程将从实际应用的角度介绍深度学习，上周课程已经学会了如何实现一个神经网络，本周将学习，实际应用中如何使神经网络高效工作。本周将学习在">
<meta property="og:type" content="article">
<meta property="og:title" content="coursera-deeplearning-ai-c2-week1">
<meta property="og:url" content="://vernlium.github.io/2018/07/05/coursera-deeplearning-ai-c2-week1/index.html">
<meta property="og:site_name" content="Vernlium">
<meta property="og:description" content="本周课程主要讲解了神经网络中的优化的一些方法，要点：  Train/Dev/Test（训练/开发/测试数据集） 偏差和方差 欠拟合和过拟合 Regularization正则化 Dropout随机失活 Normalizing归一化 梯度消失和梯度爆炸 梯度检查  本周课程将从实际应用的角度介绍深度学习，上周课程已经学会了如何实现一个神经网络，本周将学习，实际应用中如何使神经网络高效工作。本周将学习在">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="/2018/07/05/coursera-deeplearning-ai-c2-week1/basic_recipe_for_machine_learning.jpg">
<meta property="og:updated_time" content="2018-08-07T00:37:23.852Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="coursera-deeplearning-ai-c2-week1">
<meta name="twitter:description" content="本周课程主要讲解了神经网络中的优化的一些方法，要点：  Train/Dev/Test（训练/开发/测试数据集） 偏差和方差 欠拟合和过拟合 Regularization正则化 Dropout随机失活 Normalizing归一化 梯度消失和梯度爆炸 梯度检查  本周课程将从实际应用的角度介绍深度学习，上周课程已经学会了如何实现一个神经网络，本周将学习，实际应用中如何使神经网络高效工作。本周将学习在">
<meta name="twitter:image" content="/2018/07/05/coursera-deeplearning-ai-c2-week1/basic_recipe_for_machine_learning.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: false,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="://vernlium.github.io/2018/07/05/coursera-deeplearning-ai-c2-week1/"/>





  <title>coursera-deeplearning-ai-c2-week1 | Vernlium</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Vernlium</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Keep codeing and thinking!</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="://vernlium.github.io/2018/07/05/coursera-deeplearning-ai-c2-week1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张阿楠">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/wukong.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Vernlium">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">coursera-deeplearning-ai-c2-week1</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-05T07:15:57+08:00">
                2018-07-05
              </time>
            

            

            
          </span>

          

          
            
            <!--noindex-->
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/05/coursera-deeplearning-ai-c2-week1/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count hc-comment-count" data-xid="2018/07/05/coursera-deeplearning-ai-c2-week1/" itemprop="commentsCount"></span>
                </a>
              </span>
              <!--/noindex-->
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本周课程主要讲解了神经网络中的优化的一些方法，要点：</p>
<ul>
<li>Train/Dev/Test（训练/开发/测试数据集）</li>
<li>偏差和方差</li>
<li>欠拟合和过拟合</li>
<li>Regularization正则化</li>
<li>Dropout随机失活</li>
<li>Normalizing归一化</li>
<li>梯度消失和梯度爆炸</li>
<li>梯度检查</li>
</ul>
<p>本周课程将从实际应用的角度介绍深度学习，上周课程已经学会了如何实现一个神经网络，本周将学习，实际应用中如何使神经网络高效工作。本周将学习在实际应用中如何使神经网络高效工作。这些方法包括超参数调整，数据准备，再到如何确保优化算法运行得足够快，以使得学习算法能在合理的时间内完成学习任务。</p>
<p><strong>学习目标</strong></p>
<ul>
<li>Recall that different types of initializations lead to different results</li>
<li>Recognize the importance of initialization in complex neural networks.</li>
<li>Recognize the difference between train/dev/test sets</li>
<li>Diagnose the bias and variance issues in your model</li>
<li>Learn when and how to use regularization methods such as dropout or L2 regularization.</li>
<li>Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with them</li>
<li>Use gradient checking to verify the correctness of your backpropagation implementation</li>
</ul>
<h2 id="课程笔记"><a href="#课程笔记" class="headerlink" title="课程笔记"></a>课程笔记</h2><h3 id="Setting-up-your-Machine-Learning-Application"><a href="#Setting-up-your-Machine-Learning-Application" class="headerlink" title="Setting up your Machine Learning Application"></a>Setting up your Machine Learning Application</h3><h4 id="Train-Dev-Test-sets"><a href="#Train-Dev-Test-sets" class="headerlink" title="Train / Dev / Test sets"></a>Train / Dev / Test sets</h4><p>数据集分为<strong>训练集</strong>、<strong>开发集</strong>和<strong>测试集</strong>。</p>
<p>假设这是你的训练数据，把它画成一个大矩形，那么传统的做法是你可能会从所有数据中，取出一部分用作训练集，然后再留出一部分作为hold-out交叉验证集（hold-out cross validation set）。这个数据集有时也称为开发集，为了简洁，把它称为”dev set”。再接下来从最后取出一部分用作测试集。</p>
<p>整个工作流程：</p>
<ul>
<li>首先不停地用<strong>训练集</strong>来训练你的算法</li>
<li>然后用你的<strong>开发集</strong>或说hold-out交叉验证集来测试，许多不同的模型里哪一个在开发集上效果最好</li>
<li>最后评估最终的训练结果，可以用<strong>测试集</strong>对结果中最好的模型进行评估，这样以使得评估算法性能时不引入偏差 </li>
</ul>
<p>在上一个时代的机器学习中，通常的分割法是，训练集和测试集分别占整体数据70%和30%。如果你明确地设定了开发集，那比例可能是60/20/20%，也就是测试集占60%，开发集20%，测试集20%，</p>
<p>数年以前这个比例被广泛认为是，机器学习中的最佳方法，如果一共只有100个样本，也许1000个样本，甚至到1万个样本时，这些比例作为最佳选择都是合理的。</p>
<p>但是在这个大数据的时代，趋势可能会变化，可能有多达100万个训练样本，而开发集，和测试集在总体数据中所占的比例就变小了，这是因为，<strong>开发集存在的意义是用来，测试不同的算法并确定哪种最好</strong>，所以开发集只要足够大到，能够用来在评估两种不同的算法，或是十种不同的算法时快速选出较好的一种，达成这个目标可能不需要多达20%的数据。所以如果有100万个训练样本，可能开发集只要1万个样本就足够了，足够用来评估两种算法中哪一种更好。与开发集相似，<strong>测试集的主要功能是，对训练好的分类器的性能，给出可信度较高的评估</strong>。同样如果可能有100万个样本，但是只要1万个，就足够评估单个分类器的性能，能够对其性能给出比较准确的估计了。</p>
<p>如果有100万个样本，而只需要1万个用作开发集，1万个用作测试集，那么1万个只是100万个的百分之一，所以比例就是98/1/1%。还有一些应用的样本可能多于100万个，分割比率可能会变成99.5/0.25/0.25%，或者开发集占0.4%，测试集占0.1%。</p>
<p>所以总结起来，<strong>当设定机器学习问题时，通常将数据分为训练集，开发集和测试集。如果数据集比较小，也许就可以采用传统的分割比率，但如果数据集大了很多，那也可以使开发集，和测试集远小于总数据20%，甚至远少于10%。</strong></p>
<p>当前的深度学习中还有一个趋势是，<strong>有越来越多的人的训练集与测试集的数据分布不匹配</strong>。假设构建一个应用，允许用户上传大量图片，目标是找出猫的图片再展示给用户，也许因为用户都是爱猫之人，而训练集可能来自网上下载的猫的图片，而开发集和测试集则包含用户用应用上传的图片，所以，一边是训练集可能有很多从网上爬取的图片，另一边是，开发集和测试集中有许多用户上传的图片，会发现许多网页上的图片都是高分辨率，专业制作过，构图也很漂亮的猫的图片，而用户上传的图则相对比较模糊，分辨率低，用手机在更加随意的情况下拍摄的，所以这可能就造成两种不同的分布，在这种情况下建议的经验法则是，<strong>确保开发集和测试集中的数据分布相同</strong>。</p>
<p><strong>Make sure that the dev and test sets come from the same distribution.</strong></p>
<p>即使没有测试集也许也是可以的。测试集的目的是给你一个无偏估计，来评价最终所选取的网络的性能。但<strong>如果不需要无偏的估计的话，没有测试集也许也没有问题</strong>。所以当只有开发集而没有测试集的时候，所做的就是用训练集尝试不同的模型结构，然后用开发集去评估它们，根据结果进一步迭代，并尝试得到一个好的模型，因为模型拟合了开发集中的数据，所以开发集不能给无偏的估计。</p>
<h4 id="Bias-Variance"><a href="#Bias-Variance" class="headerlink" title="Bias / Variance"></a>Bias / Variance</h4><ul>
<li>Bias，偏差，描述偏离度。</li>
<li>Variance，方差，描述集中度。</li>
</ul>

<p>坐标中的⭕和×表示训练集。</p>
<p>左侧是用一条直线来区分样本数据，用逻辑回归可能画出图上的这条直线，这和训练数据的拟合度并不高，这样的分类我们称之为<strong>高偏差</strong>。或者换一种说法，称为<strong>欠拟合</strong>。</p>
<p>相对的，右侧的曲线，如果使用一个极为复杂的分类器，或许可以像图上画的这样完美区分训练数据，但看上去也并不是一个非常好的分类算法 这个<strong>高方差</strong>的分类器，我们也称之为<strong>过拟合</strong> 。</p>
<p>中间的一条曲线是比较合适的。</p>

<p>训练集的误差，至少可以知道算法是否可以很好的拟合训练集数据，然后总结出是否属于高偏差问题。然后通过观察同一个算法，在开发集上的误差了多少，可以知道这个算法是否有高方差问题。这样就能判断训练集上的算法是否在开发集上同样适用。上述结果都基于贝叶斯误差非常低，并且训练集和开发集都来自与同一个分布。</p>
<p>高偏差高方差的例子如下：</p>

<p>通过观察算法，在训练集和开发集的误差来诊断，它是否有高偏差或者高方差的问题，或许两者都有，或许都没有，基于算法遇到高偏差或高方差问题的不同情况，可以尝试不同的方法来进行改进。</p>
<h4 id="Basic-Recipe-for-Machine-Learning"><a href="#Basic-Recipe-for-Machine-Learning" class="headerlink" title="Basic Recipe for Machine Learning"></a>Basic Recipe for Machine Learning</h4><img src="/2018/07/05/coursera-deeplearning-ai-c2-week1/basic_recipe_for_machine_learning.jpg" alt="basic recipe for machine learning" title="basic recipe for machine learning">
<p>图中，找到一个新的神经网络结构，这个办法可能有效，也可能无效。把它写在括号里，是因为它是一种需要你亲自尝试的方法，也许最终能使它有效，也许不能。</p>
<p>但在当前这个深度学习和大数据的时代,只要能不断扩大所训练的网络的规模,只要能不断获得更多数据,虽然这两点都不是永远成立的,但如果这两点是可能的,那扩大网络几乎总是能够,减小偏差而不增大方差。只要用恰当的方式正则化的话，而获得更多数据几乎总是能够，减小方差而不增大偏差。</p>
<p>所以归根结底，有了这两步以后，再加上能够选取不同的网络来训练，以及获取更多数据的能力，就有了能够且只单独削减偏差或者能够并且单独削减方差，同时不会过多影响另一个指标的能力。这就是诸多原因中的一个，能够解释为何深度学习在监督学习中如此有用，以及为何在深度学习中，偏差与方差的权衡要不明显得多。这样你就不需小心地平衡两者，而是因为有了更多选择，可以单独削减偏差或单独削减方差，而不会同时增加方差或偏差。而且事实上当有了一个良好地正则化的网络时，训练一个更大的网络几乎从来没有坏处，当训练的神经网络太大时主要的代价只是计算时间。</p>
<h3 id="Regularizing-your-neural-network"><a href="#Regularizing-your-neural-network" class="headerlink" title="Regularizing your neural network"></a>Regularizing your neural network</h3><h4 id="Regularization-正则化"><a href="#Regularization-正则化" class="headerlink" title="Regularization(正则化)"></a>Regularization(正则化)</h4><h5 id="逻辑回归的正则化"><a href="#逻辑回归的正则化" class="headerlink" title="逻辑回归的正则化"></a>逻辑回归的正则化</h5><script type="math/tex; mode=display">
\min_{w,b} J(w,b)</script><p>正则化前：</p>
<script type="math/tex; mode=display">
J(w,b) = -\frac{1}{m}\sum_{i=1}^{m}[\mathcal{L}(\hat{y}^{(i)}, y^{(i)})]</script><p>$L_2$正则化后：</p>
<script type="math/tex; mode=display">
J(w,b) = -\frac{1}{m}\sum_{i=1}^{m}[\mathcal{L}(\hat{y}^{(i)}, y^{(i)})] + \frac{\lambda}{2m} \parallel w \parallel_2^2</script><p>其中，$\lambda$是正则化参数。</p>
<p>为什么只对参数w进行正则化呢? 为什么不把b的相关项也加进去呢？实际上可以这样做,但通常会把它省略掉。因为参数$w$往往是一个非常高维的参数矢量,尤其是在发生高方差问题的情况下,可能w有非常多的参数。而b只是单个数字，几乎所有的参数都集中在w中，而不是b中。即使你加上了最后这一项，实际上也不会起到太大的作用，因为b只是大量参数中的一个参数，在实践中通常就不费力气去包含它了。</p>
<p>$ L_2$正则化：</p>
<script type="math/tex; mode=display">\frac{\lambda}{2m} \parallel w \parallel_2^2 = \frac{\lambda}{2m}\sum_{i=1}^{m}{w_j^2} =\frac{\lambda}{2m} w^Tw</script><p>$ L_2$正则化是参数矢量w的欧几里得范数的平方。</p>
<p>$ L_1 $正则化：</p>
<script type="math/tex; mode=display">\frac{\lambda}{2m} \parallel w \parallel_1 = \frac{\lambda}{2m} \sum_{i=1}^{m}{|w_j|}</script><p>如果使用$ L_1 $正则化，最终$w$会变的稀疏（sparse），也就是包含很多0. 有些人认为这有助于压缩模型，因为有一部分参数是0，只需较少的内存来存储模型。然而在实践中发现，通过L1正则化让模型变得稀疏，带来的收效甚微。所以在压缩模型的目标上，它的作用不大，在训练网络时，L2正则化使用得频繁得多。 </p>
<blockquote>
<p>注：$L<em>1$范数，表示向量中每个元素绝对值的和：$$ \parallel x \parallel_1 =  \sum</em>{i=1}^{m}{|x<em>i|} <script type="math/tex">$L_2$范数,也称为欧几里得距离：</script> \parallel x \parallel_2 =  \sqrt{\sum</em>{i=1}^{m}{x_i^2}} $$<br>L2范数越小，可以使得x的每个元素都很小，接近于0。在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它<strong>权值衰减（Weight Decay）</strong>。越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。</p>
</blockquote>
<h5 id="神经网络的正则化"><a href="#神经网络的正则化" class="headerlink" title="神经网络的正则化"></a>神经网络的正则化</h5><p>正则化前：</p>
<script type="math/tex; mode=display">
J(w^{[1]},b^{[1]},\cdots,w^{[l]},b^{[l]}) = -\frac{1}{m}\sum_{i=1}^{m}[\mathcal{L}(\hat{y}^{(i)}, y^{(i)})]</script><p>正则化后：</p>
<script type="math/tex; mode=display">
J(w^{[1]},b^{[1]},\cdots,w^{[l]},b^{[l]}) = -\frac{1}{m}\sum_{i=1}^{m}[\mathcal{L}(\hat{y}^{(i)}, y^{(i)})] + \frac{\lambda}{2m} \sum_{i=1}^{l} \parallel w^{[l]} \parallel_F^2</script><p>其中，</p>
<script type="math/tex; mode=display">
 \parallel w^{[l]} \parallel_F^2 = \sum_{i=1}^{n^{[l]}}\sum_{i=j}^{n^{[l-1]}}(w_{ij}^{[l]})^2</script><p>这里矩阵范数的平方定义为 对于i和j 对矩阵中每一个元素的平方求和 如果你想为这个求和加上索引 这个求和是i从1到n[l-1] j从1到n[l] 因为w是一个n[l-1]列 n[l]行的矩阵 这些是第l-1层和第l层的隐藏单元数量单元数量 或 这个矩阵的范数 称为矩阵的<strong>弗罗贝尼乌斯范数</strong></p>
<p>$\lambda$是正则化参数。</p>
<h4 id="Why-regularization-reduces-overfitting"><a href="#Why-regularization-reduces-overfitting" class="headerlink" title="Why regularization reduces overfitting?"></a>Why regularization reduces overfitting?</h4><h4 id="Dropout-Regularization"><a href="#Dropout-Regularization" class="headerlink" title="Dropout Regularization"></a>Dropout Regularization</h4><h4 id="Understanding-Dropout"><a href="#Understanding-Dropout" class="headerlink" title="Understanding Dropout"></a>Understanding Dropout</h4><h4 id="Other-regularization-methods"><a href="#Other-regularization-methods" class="headerlink" title="Other regularization methods"></a>Other regularization methods</h4><p>Dropout regularizetion(随机失活正则法（丢弃法）)</p>
<p>dropout的实现：</p>
<p>Inverted dropout:反向随机失活，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">layer l = 3</div><div class="line"></div><div class="line">keep-prob = 0.8  # 保留网络中某一节点的概率值，0.8意味着layer中的节点有20%的概率被丢弃</div><div class="line"></div><div class="line">d3 = np.random.rand(a3.shape[0],a3.shape[1] ) &lt; keep-prob) ? 1 : 0</div><div class="line"></div><div class="line">a3 = np.multiply(a3,d3)  # 把某些元素置为0，表示此节点被隐藏</div><div class="line"></div><div class="line">a3 /= keep-prob</div></pre></td></tr></table></figure>
<p>最后一步的作用是”保证下一层 $Z^{[4]} = W^{[4]}  a^{[3]} + b^{[4]}$ 的期望值不会降低。</p>
<p><strong>如果觉得某一层比其他层更容易发生过拟合，给这一层设置更低的保留率（keep-prop）</strong></p>
<p>测试阶段：No dropout</p>
<p>为什么dropout有效： <strong>Can’t rely on any one feature,so have to spread out weights.</strong></p>
<p>shrink weights 压缩权重的平方范数。</p>
<h3 id="Setting-up-your-optimization-problem"><a href="#Setting-up-your-optimization-problem" class="headerlink" title="Setting up your optimization problem"></a>Setting up your optimization problem</h3><h4 id="Normalizing-inputs"><a href="#Normalizing-inputs" class="headerlink" title="Normalizing inputs"></a>Normalizing inputs</h4><h4 id="Vanishing-Exploding-gradients"><a href="#Vanishing-Exploding-gradients" class="headerlink" title="Vanishing / Exploding gradients"></a>Vanishing / Exploding gradients</h4><h4 id="Weight-Initialization-for-Deep-Networks"><a href="#Weight-Initialization-for-Deep-Networks" class="headerlink" title="Weight Initialization for Deep Networks"></a>Weight Initialization for Deep Networks</h4><h4 id="Numerical-approximation-of-gradients"><a href="#Numerical-approximation-of-gradients" class="headerlink" title="Numerical approximation of gradients"></a>Numerical approximation of gradients</h4><h4 id="Gradient-checking"><a href="#Gradient-checking" class="headerlink" title="Gradient checking"></a>Gradient checking</h4><h4 id="Gradient-Checking-Implementation-Notes"><a href="#Gradient-Checking-Implementation-Notes" class="headerlink" title="Gradient Checking Implementation Notes"></a>Gradient Checking Implementation Notes</h4><p>normalizing归一化</p>
<ol>
<li>均值归零：<script type="math/tex; mode=display">\mu = \frac{1}{m} \sum_{i=1}^m x^{(i)} \,\,\,\,\,\,\,\,\,  x= x - \mu</script></li>
<li>方差归一化： <script type="math/tex; mode=display">\sigma ^ {2} = \frac{1}{m} \sum_{i=1}^m (x^{(i)})^2  \,\,\,\,  x=   \frac{x}{\sigma ^2}</script></li>
</ol>
<h3 id="编程练习"><a href="#编程练习" class="headerlink" title="编程练习"></a>编程练习</h3><h4 id="编程作业-Initialization"><a href="#编程作业-Initialization" class="headerlink" title="编程作业: Initialization"></a>编程作业: Initialization</h4><h4 id="编程作业-Regularization"><a href="#编程作业-Regularization" class="headerlink" title="编程作业: Regularization"></a>编程作业: Regularization</h4><h4 id="编程作业-Gradient-Checking"><a href="#编程作业-Gradient-Checking" class="headerlink" title="编程作业: Gradient Checking"></a>编程作业: Gradient Checking</h4>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/24/coursera中无法播放视频解决方法/" rel="next" title="coursera中无法播放视频解决方法">
                <i class="fa fa-chevron-left"></i> coursera中无法播放视频解决方法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/05/coursera-deeplearning-ai-c2-week2/" rel="prev" title="coursera-deeplearning-ai-c2-week2">
                coursera-deeplearning-ai-c2-week2 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="hypercomments_widget"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/wukong.png"
               alt="张阿楠" />
          <p class="site-author-name" itemprop="name">张阿楠</p>
           
              <p class="site-description motion-element" itemprop="description">Keep codeing and thinking!</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">53</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">26</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/vernlium" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.zhihu.com/people/ananzhang" target="_blank" title="ZhiHu">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      ZhiHu
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#课程笔记"><span class="nav-number">1.</span> <span class="nav-text">课程笔记</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Setting-up-your-Machine-Learning-Application"><span class="nav-number">1.1.</span> <span class="nav-text">Setting up your Machine Learning Application</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Train-Dev-Test-sets"><span class="nav-number">1.1.1.</span> <span class="nav-text">Train / Dev / Test sets</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bias-Variance"><span class="nav-number">1.1.2.</span> <span class="nav-text">Bias / Variance</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Basic-Recipe-for-Machine-Learning"><span class="nav-number">1.1.3.</span> <span class="nav-text">Basic Recipe for Machine Learning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularizing-your-neural-network"><span class="nav-number">1.2.</span> <span class="nav-text">Regularizing your neural network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Regularization-正则化"><span class="nav-number">1.2.1.</span> <span class="nav-text">Regularization(正则化)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#逻辑回归的正则化"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">逻辑回归的正则化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#神经网络的正则化"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">神经网络的正则化</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Why-regularization-reduces-overfitting"><span class="nav-number">1.2.2.</span> <span class="nav-text">Why regularization reduces overfitting?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dropout-Regularization"><span class="nav-number">1.2.3.</span> <span class="nav-text">Dropout Regularization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Understanding-Dropout"><span class="nav-number">1.2.4.</span> <span class="nav-text">Understanding Dropout</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Other-regularization-methods"><span class="nav-number">1.2.5.</span> <span class="nav-text">Other regularization methods</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Setting-up-your-optimization-problem"><span class="nav-number">1.3.</span> <span class="nav-text">Setting up your optimization problem</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Normalizing-inputs"><span class="nav-number">1.3.1.</span> <span class="nav-text">Normalizing inputs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Vanishing-Exploding-gradients"><span class="nav-number">1.3.2.</span> <span class="nav-text">Vanishing / Exploding gradients</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Weight-Initialization-for-Deep-Networks"><span class="nav-number">1.3.3.</span> <span class="nav-text">Weight Initialization for Deep Networks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Numerical-approximation-of-gradients"><span class="nav-number">1.3.4.</span> <span class="nav-text">Numerical approximation of gradients</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient-checking"><span class="nav-number">1.3.5.</span> <span class="nav-text">Gradient checking</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient-Checking-Implementation-Notes"><span class="nav-number">1.3.6.</span> <span class="nav-text">Gradient Checking Implementation Notes</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编程练习"><span class="nav-number">1.4.</span> <span class="nav-text">编程练习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#编程作业-Initialization"><span class="nav-number">1.4.1.</span> <span class="nav-text">编程作业: Initialization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#编程作业-Regularization"><span class="nav-number">1.4.2.</span> <span class="nav-text">编程作业: Regularization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#编程作业-Gradient-Checking"><span class="nav-number">1.4.3.</span> <span class="nav-text">编程作业: Gradient Checking</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张阿楠</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	

		<script type="text/javascript">
		_hcwp = window._hcwp || [];

		_hcwp.push({widget:"Bloggerstream", widget_id: 95829, selector:".hc-comment-count", label: "{\%COUNT%\}" });

		
		_hcwp.push({widget:"Stream", widget_id: 95829, xid: "2018/07/05/coursera-deeplearning-ai-c2-week1/"});
		

		(function() {
		if("HC_LOAD_INIT" in window)return;
		HC_LOAD_INIT = true;
		var lang = (navigator.language || navigator.systemLanguage || navigator.userLanguage || "en").substr(0, 2).toLowerCase();
		var hcc = document.createElement("script"); hcc.type = "text/javascript"; hcc.async = true;
		hcc.src = ("https:" == document.location.protocol ? "https" : "http")+"://w.hypercomments.com/widget/hc/95829/"+lang+"/widget.js";
		var s = document.getElementsByTagName("script")[0];
		s.parentNode.insertBefore(hcc, s.nextSibling);
		})();
		</script>

	










  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
